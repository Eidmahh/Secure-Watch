import cv2
import numpy as np
import mediapipe as mp  # Re-enabled for Python 3.11.3
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
import tensorflow as tf  # Re-enabled for Python 3.11.3
from tensorflow.keras.models import load_model
from tensorflow.keras.layers import Layer
import os
import urllib.request
from collections import deque
import time
import threading

# Check library availability
try:
    import tensorflow as tf
    from tensorflow.keras.layers import Layer
    TENSORFLOW_AVAILABLE = True
    print("‚úÖ TensorFlow available")
except ImportError:
    TENSORFLOW_AVAILABLE = False
    print("‚ùå TensorFlow not available")

try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
    print("‚úÖ MediaPipe available")
except ImportError:
    MEDIAPIPE_AVAILABLE = False
    print("‚ùå MediaPipe not available")

# Custom Layer Classes - Python 3.13 Compatible
try:
    # Try to import TensorFlow components
    import tensorflow as tf
    from tensorflow.keras.layers import Layer
    
    class SqueezeLayer(Layer):
        def __init__(self, axis, **kwargs):
            super().__init__(**kwargs)
            self.axis = axis
        
        def call(self, inputs):
            return tf.squeeze(inputs, axis=self.axis)
        
        def get_config(self):
            config = super().get_config()
            config.update({"axis": self.axis})
            return config

    class ExpandDimsLayer(Layer):
        def __init__(self, axis, **kwargs):
            super().__init__(**kwargs)
            self.axis = axis
        
        def call(self, inputs):
            return tf.expand_dims(inputs, axis=self.axis)
        
        def get_config(self):
            config = super().get_config()
            config.update({"axis": self.axis})
            return config

    class MultiplyLayer(Layer):
        def call(self, inputs):
            return inputs[0] * inputs[1]

    class ReduceSumLayer(Layer):
        def __init__(self, axis, **kwargs):
            super().__init__(**kwargs)
            self.axis = axis
        
        def call(self, inputs):
            return tf.reduce_sum(inputs, axis=self.axis)
        
        def get_config(self):
            config = super().get_config()
            config.update({"axis": self.axis})
            return config
    
    TENSORFLOW_AVAILABLE = True
    
except ImportError:
    print("‚ö†Ô∏è TensorFlow not available - using fallback mode")
    TENSORFLOW_AVAILABLE = False
    
    # Create dummy classes for compatibility
    class Layer:
        pass
    
    class SqueezeLayer(Layer):
        def __init__(self, *args, **kwargs):
            pass
    
    class ExpandDimsLayer(Layer):
        def __init__(self, *args, **kwargs):
            pass
    
    class MultiplyLayer(Layer):
        def __init__(self, *args, **kwargs):
            pass
    
    class ReduceSumLayer(Layer):
        def __init__(self, *args, **kwargs):
            pass

class ViolenceDetector:
    
    def __init__(self, model_path=None, pose_model_path=None, sequence_length=15):
        self.sequence_length = sequence_length
        self.model_path = model_path or 'models/violence_detector_final.keras'
        self.pose_model_path = pose_model_path or 'models/pose_landmarker.task'
        self.model = None
        self.pose = None
        self.mp_pose = None
        self.previous_positions = {}
        self.frame_buffer = deque(maxlen=sequence_length)
        self.prediction_history = deque(maxlen=10)
        self.is_running = False
        
        # Try to initialize MediaPipe (skip if not available)
        try:
            import mediapipe as mp
            self.mp_pose = mp.solutions.pose
            print("‚úÖ MediaPipe initialized for pose detection")
        except:
            self.mp_pose = None
            print("‚ö†Ô∏è MediaPipe not available - using basic motion detection")
        
        # Initialize basic motion detection as fallback
        self.previous_frame = None
        self.motion_history = deque(maxlen=30)
        
        # Try to setup MediaPipe (will gracefully fail)
        self._setup_mediapipe()
        
        # Try to load AI model (will create fallback if needed)
        self._load_model()
    
    def _setup_mediapipe(self):
        try:
            # Download pose model if not exists
            if not os.path.exists(self.pose_model_path):
                os.makedirs(os.path.dirname(self.pose_model_path), exist_ok=True)
                print("üì• Downloading pose model...")
                model_url = "https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task"
                urllib.request.urlretrieve(model_url, self.pose_model_path)
                print("‚úÖ Pose model downloaded!")
            
            # Initialize MediaPipe with options
            base_options = python.BaseOptions(model_asset_path=self.pose_model_path)
            options = vision.PoseLandmarkerOptions(
                base_options=base_options,
                running_mode=vision.RunningMode.IMAGE,
                num_poses=5,
                min_pose_detection_confidence=0.5
            )
            self.pose = vision.PoseLandmarker.create_from_options(options)
            print("‚úÖ MediaPipe pose landmarker initialized!")
            
        except Exception as e:
            print(f"‚ö†Ô∏è MediaPipe setup failed: {e}")
            self.pose = None
    
    def _load_model(self):
        try:
            print(f" Looking for model at: {self.model_path}")
            print(f" Current directory: {os.getcwd()}")
            
            if not TENSORFLOW_AVAILABLE:
                print(" TensorFlow not available - using motion detection fallback")
                self.model = None
                return
            
            if os.path.exists(self.model_path):
                print(f"‚úÖ Model file found: {self.model_path}")
                if self.model_path.endswith('.keras'):
                    from tensorflow.keras.models import load_model
                    self.model = load_model(
                        self.model_path,
                        custom_objects={
                            'SqueezeLayer': SqueezeLayer,
                            'ExpandDimsLayer': ExpandDimsLayer,
                            'MultiplyLayer': MultiplyLayer,
                            'ReduceSumLayer': ReduceSumLayer
                        }
                    )
                    print("AI violence detection model loaded successfully!")
                    print(f" Model input shape: {self.model.input_shape}")
                    return  # Exit early on success
                else:
                    print(f"‚ö†Ô∏è Model file format not supported: {self.model_path}")
            else:
                print(f"‚ö†Ô∏è Model file not found: {self.model_path}")
            
            # If we get here, create fallback model
            print("üîß Creating simple fallback model...")
            self._create_simple_model()
                    
        except Exception as e:
            print(f"‚ö†Ô∏è Model loading error: {e}")
            print("üîß Using motion detection fallback...")
            self.model = None
    
    def _create_simple_model(self):
        try:
            from tensorflow.keras.models import Sequential
            from tensorflow.keras.layers import LSTM, Dense, Dropout
            
            print(" Creating simple violence detection model...")
            model = Sequential([
                LSTM(64, input_shape=(self.sequence_length, 10)),
                Dense(32, activation='relu'),
                Dropout(0.3),
                Dense(1, activation='sigmoid')
            ])
            
            model.compile(
                optimizer='adam',
                loss='binary_crossentropy',
                metrics=['accuracy']
            )
            X_dummy = np.random.random((10, self.sequence_length, 10))
            y_dummy = np.random.randint(0, 2, (10, 1))
            model.fit(X_dummy, y_dummy, epochs=1, verbose=0)
            os.makedirs('models', exist_ok=True)
            model.save(self.model_path)
            self.model = model
            print(" Simple model created and loaded!")
            
        except Exception as e:
            print(f"Failed to create simple model: {e}")
            self.model = None
    
    def extract_person_features(self, landmarks_dict, person_id=0):
        try:
            def get_angle(p1, p2, p3):
                v1 = p1 - p2
                v2 = p3 - p2
                norm1, norm2 = np.linalg.norm(v1), np.linalg.norm(v2)
                if norm1 == 0 or norm2 == 0:
                    return 0.0
                cos_angle = np.dot(v1, v2) / (norm1 * norm2)
                cos_angle = np.clip(cos_angle, -1.0, 1.0)
                angle = np.arccos(cos_angle)
                return np.degrees(angle)
            required_landmarks = [
                'LEFT_SHOULDER', 'LEFT_ELBOW', 'LEFT_WRIST',
                'RIGHT_SHOULDER', 'RIGHT_ELBOW', 'RIGHT_WRIST',
                'LEFT_HIP', 'LEFT_KNEE', 'LEFT_ANKLE',
                'RIGHT_HIP', 'RIGHT_KNEE', 'RIGHT_ANKLE'
            ]
            
            for landmark in required_landmarks:
                if landmark not in landmarks_dict:
                    print(f"‚ö†Ô∏è Missing landmark: {landmark}")
                    return None, None
            
            # Extract key angles and measurements
            left_arm_angle = get_angle(
                landmarks_dict['LEFT_SHOULDER'],
                landmarks_dict['LEFT_ELBOW'],
                landmarks_dict['LEFT_WRIST']
            )
            right_arm_angle = get_angle(
                landmarks_dict['RIGHT_SHOULDER'],
                landmarks_dict['RIGHT_ELBOW'],
                landmarks_dict['RIGHT_WRIST']
            )
            left_leg_angle = get_angle(
                landmarks_dict['LEFT_HIP'],
                landmarks_dict['LEFT_KNEE'],
                landmarks_dict['LEFT_ANKLE']
            )
            right_leg_angle = get_angle(
                landmarks_dict['RIGHT_HIP'],
                landmarks_dict['RIGHT_KNEE'],
                landmarks_dict['RIGHT_ANKLE']
            )
            
            # Body measurements
            shoulder_width = np.linalg.norm(landmarks_dict['RIGHT_SHOULDER'] - landmarks_dict['LEFT_SHOULDER'])
            hip_width = np.linalg.norm(landmarks_dict['RIGHT_HIP'] - landmarks_dict['LEFT_HIP'])
            shoulder_hip_ratio = shoulder_width / (hip_width + 1e-6)
            
            # Movement velocity
            center = (landmarks_dict['LEFT_HIP'] + landmarks_dict['RIGHT_HIP']) / 2
            velocity = np.linalg.norm(center - self.previous_positions.get(person_id, center))
            self.previous_positions[person_id] = center
            
            # Return exactly 10 features (matching model input)
            features = np.array([
                left_arm_angle, right_arm_angle,
                left_leg_angle, right_leg_angle,
                shoulder_hip_ratio, velocity,
                shoulder_width, hip_width,
                center[0], center[1]  # Position features
            ])
            
            # Ensure no NaN or inf values
            features = np.nan_to_num(features, nan=0.0, posinf=180.0, neginf=0.0)
            
            return features, center
            
        except Exception as e:
            print(f"Feature extraction error: {e}")
            return None, None
    
    def detect_violence_in_frame(self, frame):
        """
        Detect violence in a single frame - Python 3.11.3 Compatible
        Returns: (violence_probability, is_violence_detected, processed_frame)
        """
        # Use pose-based detection if both model and pose detector are available
        if self.model is not None and self.pose is not None:
            return self._pose_based_detection(frame)
        
        # Fall back to motion detection if AI models aren't available
        return self._motion_based_detection(frame)
    
    def _motion_based_detection(self, frame):
       
        try:
           
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            gray = cv2.GaussianBlur(gray, (21, 21), 0)
            
           
            if self.previous_frame is None or self.previous_frame.shape != gray.shape:
                print(f"üîÑ Initializing motion detection for frame size: {gray.shape}")
                self.previous_frame = gray.copy()
                self.motion_history.clear()
                return 0.1, False, frame
            
            if self.previous_frame.shape != gray.shape:
                print(f"‚ö†Ô∏è Frame size mismatch: previous {self.previous_frame.shape} vs current {gray.shape}")
                self.previous_frame = gray.copy()
                self.motion_history.clear()
                return 0.1, False, frame
            
            frame_delta = cv2.absdiff(self.previous_frame, gray)
            thresh = cv2.threshold(frame_delta, 30, 255, cv2.THRESH_BINARY)[1] 
            thresh = cv2.dilate(thresh, None, iterations=2)
            
            contours, _ = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            total_motion_area = sum(cv2.contourArea(c) for c in contours if cv2.contourArea(c) > 1000) 
            motion_intensity = min(total_motion_area / 50000, 1.0)  
            self.motion_history.append(motion_intensity)
        
            if len(self.motion_history) >= 15:  
                recent_motion = list(self.motion_history)[-15:]
                avg_motion = np.mean(recent_motion)
                motion_variance = np.var(recent_motion)
                max_motion = np.max(recent_motion)
                base_probability = avg_motion * 0.3 + motion_variance * 0.4 + max_motion * 0.3
                
                if avg_motion < 0.2:  # Too little motion
                    base_probability *= 0.1
                elif motion_variance < 0.05:  # Too uniform motion
                    base_probability *= 0.3
                elif max_motion < 0.4:  # No significant peaks
                    base_probability *= 0.2
                
                violence_probability = min(base_probability, 0.9)  # Cap at 90%
                is_violence = violence_probability > 0.75  # Keep same threshold
                
                # Draw motion areas on frame for visualization
                processed_frame = frame.copy()
                significant_contours = 0
                for contour in contours:
                    if cv2.contourArea(contour) > 1000:
                        significant_contours += 1
                        (x, y, w, h) = cv2.boundingRect(contour)
                        color = (0, 0, 255) if is_violence else (0, 255, 0)
                        cv2.rectangle(processed_frame, (x, y), (x + w, y + h), color, 2)
                
                # Add detailed status text
                status_text = f"Motion: {avg_motion:.3f} | Var: {motion_variance:.3f} | Violence: {violence_probability:.3f}"
                cv2.putText(processed_frame, status_text, (10, 30), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                
                detail_text = f"Areas: {significant_contours} | Max: {max_motion:.3f}"
                cv2.putText(processed_frame, detail_text, (10, 55), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                
                if is_violence:
                    cv2.putText(processed_frame, "VIOLENCE DETECTED!", (10, 85), 
                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)
                
                # Update previous frame for next comparison
                self.previous_frame = gray.copy()
                return float(violence_probability), is_violence, processed_frame
            
            # Not enough data yet
            self.previous_frame = gray.copy()
            return 0.1, False, frame
            
        except Exception as e:
            print(f"Motion detection error: {e}")
            # Reset previous frame on any error to prevent persistent issues
            self.previous_frame = None
            return 0.1, False, frame
    
    def _pose_based_detection(self, frame):
        """
        Advanced pose-based violence detection using MediaPipe and AI model
        Returns: (violence_probability, is_violence_detected, processed_frame)
        """
        try:
            # Convert frame for MediaPipe
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)
            
            # Detect poses
            pose_result = self.pose.detect(mp_image)
            
            processed_frame = frame.copy()
            
            if pose_result.pose_landmarks:
                # Extract features from all detected poses
                all_features = []
                
                for i, pose_landmarks in enumerate(pose_result.pose_landmarks):
                    # Convert landmarks to dictionary format
                    landmarks_dict = {}
                    landmark_names = [
                        'NOSE', 'LEFT_EYE_INNER', 'LEFT_EYE', 'LEFT_EYE_OUTER',
                        'RIGHT_EYE_INNER', 'RIGHT_EYE', 'RIGHT_EYE_OUTER',
                        'LEFT_EAR', 'RIGHT_EAR', 'MOUTH_LEFT', 'MOUTH_RIGHT',
                        'LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_ELBOW', 'RIGHT_ELBOW',
                        'LEFT_WRIST', 'RIGHT_WRIST', 'LEFT_PINKY', 'RIGHT_PINKY',
                        'LEFT_INDEX', 'RIGHT_INDEX', 'LEFT_THUMB', 'RIGHT_THUMB',
                        'LEFT_HIP', 'RIGHT_HIP', 'LEFT_KNEE', 'RIGHT_KNEE',
                        'LEFT_ANKLE', 'RIGHT_ANKLE', 'LEFT_HEEL', 'RIGHT_HEEL',
                        'LEFT_FOOT_INDEX', 'RIGHT_FOOT_INDEX'
                    ]
                    
                    for j, landmark in enumerate(pose_landmarks):
                        if j < len(landmark_names):
                            landmarks_dict[landmark_names[j]] = np.array([landmark.x, landmark.y])
                    
                    # Extract features for this person
                    features, center = self.extract_person_features(landmarks_dict, i)
                    
                    if features is not None:
                        all_features.append(features)
                        
                        # Draw pose landmarks on frame
                        self._draw_pose_landmarks(processed_frame, pose_landmarks)
                        
                        # Draw bounding box around person
                        h, w = frame.shape[:2]
                        x_coords = [lm.x * w for lm in pose_landmarks]
                        y_coords = [lm.y * h for lm in pose_landmarks]
                        
                        if x_coords and y_coords:
                            x_min, x_max = int(min(x_coords)), int(max(x_coords))
                            y_min, y_max = int(min(y_coords)), int(max(y_coords))
                            
                            # Expand bounding box slightly
                            margin = 20
                            x_min = max(0, x_min - margin)
                            y_min = max(0, y_min - margin)
                            x_max = min(w, x_max + margin)
                            y_max = min(h, y_max + margin)
                            
                            cv2.rectangle(processed_frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)
                
                # Analyze features with AI model if we have enough data
                if all_features:
                    # Add features to buffer
                    self.frame_buffer.append(all_features[0])  # Use first person for now
                    
                    if len(self.frame_buffer) >= self.sequence_length:
                        # Prepare sequence for model
                        sequence = np.array(list(self.frame_buffer))
                        sequence = sequence.reshape(1, self.sequence_length, -1)
                        
                        # Predict violence probability
                        prediction = self.model.predict(sequence, verbose=0)[0][0]
                        violence_probability = float(prediction)
                        
                        # Add to prediction history for smoothing
                        self.prediction_history.append(violence_probability)
                        
                        # Use smoothed prediction
                        smoothed_probability = np.mean(list(self.prediction_history))
                        is_violence = smoothed_probability > 0.5
                        
                        # Add status information to frame
                        status_text = f"Pose-based AI Detection | Confidence: {smoothed_probability:.3f}"
                        cv2.putText(processed_frame, status_text, (10, 30), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                        
                        pose_text = f"Poses detected: {len(pose_result.pose_landmarks)} | Buffer: {len(self.frame_buffer)}"
                        cv2.putText(processed_frame, pose_text, (10, 55), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                        
                        if is_violence:
                            cv2.putText(processed_frame, "VIOLENCE DETECTED!", (10, 85), 
                                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)
                        
                        return smoothed_probability, is_violence, processed_frame
                    else:
                        # Not enough frames yet
                        status_text = f"Building sequence... {len(self.frame_buffer)}/{self.sequence_length}"
                        cv2.putText(processed_frame, status_text, (10, 30), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
                        return 0.1, False, processed_frame
            
            # No poses detected
            status_text = "No poses detected - motion detection fallback"
            #print('HELO ')
            cv2.putText(processed_frame, status_text, (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
            return self._motion_based_detection(frame)
            
        except Exception as e:
            print(f"Pose detection error: {e}")
            return self._motion_based_detection(frame)
    
    def _draw_pose_landmarks(self, image, landmarks):
        """Draw pose landmarks on the image"""
        h, w = image.shape[:2]
        
        # Draw key body landmarks
        key_points = [11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 27, 28]  # Main body points
        
        for i in key_points:
            if i < len(landmarks):
                landmark = landmarks[i]
                x, y = int(landmark.x * w), int(landmark.y * h)
                cv2.circle(image, (x, y), 5, (0, 255, 0), -1)
        
        # Draw connections between landmarks
        connections = [
            (11, 12), (11, 13), (12, 14), (13, 15), (14, 16),  # Arms
            (11, 23), (12, 24), (23, 24),  # Torso
            (23, 25), (24, 26), (25, 27), (26, 28)  # Legs
        ]
        
        for start_idx, end_idx in connections:
            if start_idx < len(landmarks) and end_idx < len(landmarks):
                start_landmark = landmarks[start_idx]
                end_landmark = landmarks[end_idx]
                
                start_x, start_y = int(start_landmark.x * w), int(start_landmark.y * h)
                end_x, end_y = int(end_landmark.x * w), int(end_landmark.y * h)
                
                cv2.line(image, (start_x, start_y), (end_x, end_y), (255, 0, 0), 2)
    
    def get_status(self):
        """Get detector status - FIXED VERSION"""
        return {
            'model_loaded': self.model is not None,
            'pose_detector_ready': self.pose is not None,
            'buffer_size': len(self.frame_buffer),
            'sequence_length': self.sequence_length,
            'is_running': self.is_running,
            'model_path': self.model_path,
            'pose_model_path': self.pose_model_path,
            # Add explicit status for API
            'status': 'operational' if self.model is not None else 'error',
            'model_file_exists': os.path.exists(self.model_path),
            'pose_file_exists': os.path.exists(self.pose_model_path)
        }

# Global detector instance
detector = None

def get_detector():
    """Get or create global detector instance"""
    global detector
    if detector is None:
        detector = ViolenceDetector()
    return detector